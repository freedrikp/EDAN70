
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

%\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{listings}   
\IEEEoverridecommandlockouts                              % This command is only needed if
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed
\usepackage{algorithm}% http://ctan.org/pkg/algorithm
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{graphicx}
\usepackage{moreverb}

\title{\LARGE \bf Project Report \\ EDAN70 Project in Computer Science \\ Intelligent Systems \\ Classifying Laser Range Data "Images" \\ Supervisor: Elin Anna Topp}
\date{\today}
\author{Fredrik Paulsson \\ dat11fp1@student.lu.se \and Shan Senanayake \\ dat11sse@student.lu.se}


\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
This report documents a project in which we developed a classifier for laser range data "images". The classifier was based on machine learning, meaning that the classifier identified measurements and classified them based on previously known classifications of measurements.

In the report we describe two approaches to solve the problem of developing this classifier. The different approaches works on different levels of abstraction, one based on individual points and one based on lines among the points. There was several problems with the classifier working among points while we had more success with the one working on points.

Because of time constraints we did not have much time to work on the second approach but in the report we list the results that we have produced and we also specify improvements that can be made in order for it to possibly work much better.


\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

In this project we were to develop a classifier that takes as input laser range data and outputs a classification of the data. The classification may be \emph{door}, \emph{open space}, \emph{clutter} and maybe \emph{wall} and similar objects.

The goal was to implement the classifier as a ROS node in C++ so that it could run online on a robot.

The project consists of several parts. The first part was to develop a parser for the laser range data that wrapped the data in suitable data structures as well as transforming the representation into a form that was easier to work with.

The second part was to develop and implement an algorithm that had the ability to classify the laser range data. This algorithm were supposed to operate within the data structures that our parser had created.

The third part was to port the classifier into a ROS node and test it in an online environment running on the robot. 

During development we used laser range data that had been gathered in the past.

This report will serve as documentation of our project.

\section{Machine Learning}
We are both very interested in machine learning and therefore we decided to develop our classifier within the machine learning area of artificial intelligence.

The idea that we had was to combine both supervised and unsupervised learning for use in order to classify the laser range data. The original idea that we had was based on manually classifying some measurements in order for the classifier to have a database to start with. When the classifier was to be used it was going to browse the database and find out which laser range data in the database that matched best with the measurement that was going to be classified. It was then going to give the new data the same classification as the data that matched best. We also envisioned that the new classification might be added to the database in order for the robot to learn.

We had discussed and thought about having a threshold on how good the best match needed to be in order for it to be used. If that threshold was crossed the classifier would either be unable to give a classification, give an "unknown" classification or simply prompt a warning that the classification might be wrong. Also we thought about having a threshold on the match that was going to be even harder to accomplish for the data than the threshold previously introduced. This threshold was simply going to control whether or not the new classification was going to be added to the database. The idea behind this was for allowing a poor match to be successfully classified without extending the database with "bad" knowledge.

However, later on in the project we changed the approach from our original idea to switch to using decision trees instead. This new idea was based on identifying characteristics in the measurements and then use these characteristics in order to classify measurements. The advantage with this approach was that we raised the abstraction level of the classifier. Instead of working on individual points it worked on patterns recognized among the points.

\section{PARSER}
The offline data of measurements that we have used to test our classifier and based the supervised learning part on was given to us in a specific format. All measurements gathered in one run of the robot's laser range scanner is put into one single file called \emph{scans.dat}. Each single measurement is placed on its own line. Measurements were gathered with a frequency of 4-5 Hz according to our supervisor.

Each of these measurements had a specific format and each measurements starts with a header of 15 values and then a value for each point in the measurement. The laser range scanner works by emitting a certain number of laser beams which will be reflected in objects in the field of view of the scanner. The scanner will then calculate the distance to each reflection point resulting in a number of distances. In each measurements the number of points and the angle between the points are given. Effectively each point's coordinates is given in polar form.

Each measurement looks like the following:

\begin{verbatim}
<unknown> <unknown> <number_of_points>
<timestamp_seconds> 
<timestamp_microseconds>
<unknown> <unknown> <unknown> <unknown>
<unknown> <unknown> 
<angle_between_points>
<unknown> <unknown> <unknown>
<distance_in_meters>^<number_of_points>
\end{verbatim}

As can be seen there are a lot of unknown values that we do not know and that our supervisor could not explain to us. However, these values are not needed by us in this project. The most important values are the number of points, the angle between the points and for each point the distance to the point.

Directly parsing the above format of each measurement yields polar coordinates. Polar coordinates are very hard to work with as they are not easily visualized or easily worked with. This made us constructing a transformation of these polar coordinates into our own Cartesian coordinate system.

Our coordinate system is based on some basic principles. The robot is positioned at origin and is the front of the robot is along the y-axis. This is basically the standard way to transform between polar and Cartesian coordinates. The only thing special about our transform is that the angle for the polar coordinates is expressed with respect to the starting point. Furthermore, the starting point may be located at different angles from the x-axis (in the Cartesian system) depending on the specific type of laser range scanner. Different scanners may have a different number of points, different angle between the points and differently sized fields of view. With field of view we mean the entire angle that the scanner can pereceive. In order to manage this problem we chose to express the angle from the y-axis instead and thus we had to substitute sine and cosine with each other in the standard transform as well as we had to explicitly manage the sign of each coordinate.

Now that we had coordinates in a much more manageable representation we could visualize the data for ourselves and we could use standard mathematical formulas in order to work with each measurement. Because we needed to manually classify measurements in order to feed these to the classifier as a starting ground it was vital that we could easily plot the measurements. This was easily done in Matlab or Octave when we had our Cartesian coordinates. Figures \ref{human}, \ref{doorhalf}, \ref{doorfull} and \ref{chair} shows some example plots.


\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/human.jpg}
\caption{This plot shows a human standing in front of the robot in a hallway. The human legs are clearly visible in the center of the plot.}
\label{human}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/doorhalf.jpg}
\caption{This plot shows a door that is half open. There is also a human standing in front of the door to the left.}
\label{doorhalf}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/doorfull.jpg}
\caption{This plot shows the same door as the plot above but now the door is fully open. Now some clutter is shown from inside the room.}
\label{doorfull}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/chair.jpg}
\caption{This plot shows the robot standing in the middle of a room facing a wall. Right in front of the robot a chair is positioned.}
\label{chair}
\end{figure}

\section{ALGORITHM}
In this section we will explain all of the algorithms we used and how they were implemented. 

We had two different approaches when trying to figure out a way to represent the data in a form onto which we could apply some form of machine learning. This was the biggest challenge we had to overcome in the project, to find an approach that made sense to use together with machine learning. 

\subsection{Interpolation and Comparison}
After transferring the data to a form in which we could more easily differentiate the characteristics of the data we decided to try an interpolation approach. This approach consisted of interpolating the data points to get a 'figure' of some sorts which we could use as a characteristic for a classifier.

The interpolation is mainly used to get an evenly distributed set of points that we could use to directly compare with another set of points in order to see how much they diferred from one another.

The idea behind this approach was to store measurements and their respective classifications in some sort of database. When the classifier then receives a measurement to classify, the classifier would iterate over the database and using the interpolation to directly compare the new measurement with all measurements stored in the database. The classification of the best match in the database would then be assigned to the new measurement. And finally the new measurement and it's classification were to be stored in the database.

This idea would combine both unsupervised and supervised machine learning. The supervised learning came into play because there would need to be a database from the start. The classifications contained in this database had to be manually classified by us. The unsupervised learning takes place because the classifier extends it's database on each run.

Possibly, the above algorithm could incorporate thresholds and a default classification. Essentially this means that if there did not exist a match that had an error less than the threshold the classifier might assign the classification unknown, prompt a warning saying that the classification was a bad match and might be wrong, or the classifier could simply refuse to give a classifiction for the given measurement. This could act as some sort of default classification and if it is used the algorithm should not add the newly classified measurement to it's database.

In order to not extend the database with measurements we thought about having an even more strict threshold than the first one. This would dictate whether the new classifiaction were going to be added to the database. This prevents classifications that just barely made it through the first threshold to extend the database with poor knowledge which could accumulate and make the classifier completely worthless.

\subsubsection{Implementation}
We decided to start the implementation with a simple linear interpolation algorithm, pseudo-code is found in algorithm \ref{lerp}.

\begin{algorithm}
  \caption{\label{lerp}Simple lerp}
  \begin{algorithmic}[1]
      \State \texttt{Dataset S}
      \State \texttt{Number of points N}
      \State \texttt{Distance increment $d$ = Total distance / N}
      \State \texttt{$i = 0$}
      \State \texttt{Point lower,upper in S}
      \State \texttt{Array of points A}
      \While{\texttt{$i <$ Total distance}}
      	\If{$lower < i < upper$}
      		\State A.add(Calculate point of ($i$));
      	\ElsIf{$lower == i$}
      		\State A.add($lower$)
      	\ElsIf{$upper == i$}
      		\State A.add($upper$)
      	\Else
      		\While{$i>higher$ and S.End}
      			\State \texttt{increment $lower$}
      			\State \texttt{increment $upper$}
      		\EndWhile
      	\EndIf
      	\State \texttt{$i += d$}
      \EndWhile
      \Return \texttt{A}
  \end{algorithmic}
\end{algorithm}

This algorithm is heavily influenced by a simple linear spline interpolation \cite{interpolation}. The \texttt{Number of points N} variable is an input to decide how many points one wants to represent the Dataset in. The \texttt{distance increment d} is used to check out where the next point should lie. \texttt{Total distance} is the total distance if you walk through all the points in the dataset (Just like connecting the dots). The \texttt{$lower < i < upper$} is to check if the distance $i$ lies in between the \texttt{lower} and \texttt{upper} points. \texttt{Calculate point of($i$)} is used to calculate the point that should lie on that distance.

\subsection{Line finding}
\label{sec:line_find}
The other approach to representing data was a bit more of an abstract approach. Instead of comparing the dataset against each other directly by using interpolation we decided to find characteristics within the dataset itself. 

The line finding algorithm does exactly what it says it finds lines in a given dataset, and given these lines we can decide different characteristics. For instance the characteristics we chose to recognize doors was how many lines there are, what is the mean length of these lines, how many are parallel against each other and finally, how many are perpendicular against each other.

The thought process behind this stems from this article \cite{legfinding} which handles how to detect legs in laser range data. We have a simple line finding algorithm, that draws a line between the first two points in a set and then uses this line to check if the next line between the first and the third point are in a given bounding box. The bounding box is determined by the simple line equation $y = kx + m$ where the upper and lower limit is determined by $y = kx + m \pm err$. 

To not make the first k-value to set the entire lines value we have decided to tweak the k-value depending on the amount of points already evaluated. This is done by calculating the weight each point has (in essence the effect) on the overall line and then updating the k value depending on the weight.

To reduce noise and wrong line finding we have determined that in order to qualify for a line in the dataset, it has to have more than 3 points from the dataset and the length of the line has to be greater than 0.25 m.
\subsubsection{Implementation}
The pseudo-code for the Line finding algorithm can be found in algorithm \ref{lineFinding}.

\begin{algorithm}
  \caption{\label{lineFinding}Line finding}
  \begin{algorithmic}[1]
      \State \texttt{Dataset S}
      \State \texttt{Error limit E}
      \State \texttt{Point start in S}
      \State \texttt{Array of lines A}
      \While{\texttt{S has unevaluated points}}
      	\State \texttt{Point p1 in S}
      	\State \texttt{$k$ = Calculate K Of (start,p1)}
      	\State \texttt{$d$ = Distance of (start,p1)}
      	\While{Is a valid line}
      		\State \texttt{Point p2 in S}
      		\State \texttt{$lower$ = calculate lower of line (start,$k$,E)}
      		\State \texttt{$higher$ = calculate higher of line (start,$k$,E)}
      		\If{Check if point is in bound ($lower$,p2,$higher$)}
      			\State \texttt{$new_k$ = calculate k of (start,p2)}
      			\State \texttt{update k ($k$,$new_k$)}
      			\State \texttt{p1 = p2}
      		\Else
      			\State \texttt{break inner loop}
      		\EndIf
      	\EndWhile
      	\If{line is valid($k$,start)}
      		\State \texttt{A.add($k$,start)}
      	\EndIf
      \EndWhile
      \Return \texttt{A}
  \end{algorithmic}
\end{algorithm}

Just as explained in section \ref{sec:line_find} the line finding algorithm takes a thick line and adjusts the k-value for each point that fits in the line. The function \texttt{Calculate k of (start,p1)} just calculates the $k$ value of two points according to the regular linear equation $k = \frac{\delta y}{\delta x}$, the functions \texttt{calculate lower of line} and \texttt{calculate higher of line} calculates how thick the lines which checks errors are. Similar the function \texttt{Check if point is in bound} simply check if the point \texttt{p2} is within the margin of error. The most interesting function is \texttt{update k} which calculates a new $k$-value depending on the previous and how many points we already have evaluated. The weight is calculated by $w = \frac{1}{number of points already evaluated}$ and the new $k$ is calculated according $k = (1-w)*k + w*(new_k)$.

\subsection{Decision Tree}
When we had identified line characteristics in our measurements as described above we had raised the level of abstraction in our data. Instead of comparing point by point against each other in order to identify patterns we could now decide based on the characteristics to which class the given measurement belongs. After some research we deduced that a decision tree was a good way to proceed within the machine learning area of artificial intelligence.

The great part of decision trees that attracted us was the fact that the input to the algorithm can be several very abstract variables. The variables can take both discrete values or real values. Decision trees also has one big advantage and that is that they are very robust against errors and noise in the training examples \cite{ml3}. The measurements that we have used can be very noise so decision trees seemed like it would work very well.

Our classification problem were going to consist of more than simply two classes. However, the decision tree outputs only a boolean, yes/no, classification for each measurement \cite{ml3}. The way we solved this problem was to use a so called one-vs-rest classifier \cite{praml}. This essentially means that we train one classifier, or in our case one decision tree, for each class that we want be able to classify. The output from each of these classifiers is simply a yes if the measurement is part of that class or no if the measurement is not part of that specific class but may be a member of the others.

\subsubsection{Implementation}
The algorithm that generates our decision tree is the well-known ID3 algorithm. Our exact implementation is based on the description and structure presented in \cite{aima}, however, a more formal presentation can be found in \cite{ml3}. As discussed in \cite{aima} our algorithm selects the attributes to split on by their information gain. In order to use real continuous values for the branches we use split-points.

For the process of acquiring good split-point values we use the process described in \cite{ml3}. The process of selecting a good split-point for a certain attribute is by sorting the examples by the attribute. The next step is to find all pairs of adjacent examples in the sorted list that differs in their classifications. For each pair, one calculates the information gain of the mean of the pair's values. Then it is simply to select the pair whose mean value maximizes the information gain.

Even though decision trees that works with continuous values on the attributes are called regression trees, our tree is not a regression tree. We never switch over to do any kind of regression. For every attribute we simply have a split-point that separates the sub-tree in two branches, a so called binary split \cite{aima}.

\section{RESULTS}


\subsection{Parser}
The result for the parser is pretty straightforward. Figures \ref{human}, \ref{doorhalf}, \ref{doorfull} and \ref{chair} have all been plotted using the Cartesian coordinates that our parser have parsed and transformed. In all these plots the robot is located at origin and is heading forward along the y-axis. Furthermore, in all plots we have narrowed the field of view to 90 degrees and we only include points within 4 meters of the robot in order to provide a more clear plot with minimum noise. However, even though we have limited the number of points plotted, our parser is able to include every point made by the laser range scanner.

\subsection{Interpolation and Comparison}
While implementing this algorithm we encountered several practical problems and eventually we realized that this was not going to work very well. Therefore, we abandoned this approach and starting researching other ways resulting in the second idea explained above. Thus, we have no results to provide from this algorithm other than the fact that we never managed to made it work as we wanted.

\subsection{Line finding}
This is the main algorithm we are using to represent datasets. We have estimated an error margin of 0.1 m (that means that the line finding algorithm uses a 0.1 m thick line) and 10 degrees of error for finding perpendicular and parallel. 

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/humanlines.jpg}
\caption{This plot shows a human standing in front of the robot in a hallway. The human legs are clearly visible in the center of the plot. The red lines are the ones our algorithm finds.}
\label{human}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/doorhalflines.jpg}
\caption{This plot shows a door that is half open. The red lines are the ones our algorithm finds. We can clearly see that the algorithm fails to find one line that seems quite valid, this is due to the boundaries that are put in.}
\label{doorhalf}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{presimg/doorfullines.jpg}
\caption{This plot shows the same door as the plot above but now the door is fully open. Now some clutter is shown from inside the room. The red lines are the ones our algorithm finds.}
\label{doorfull}
\end{figure}

\subsection{Decision Tree}
The decision tree is built up from the use of the line-finding algorithm. The attributes that are chosen are, mean length of lines, number of lines, number of parallel and number of perpendicular. This tree was built from the parameters 0.1 m thick line, 10 degrees of failure for perpendicular and parallel, a line has to be longer than 0.25 m and it has to contain at least 3 points.

\begin{lstlisting}
avglength = <=0.774125
 nbroflines = <=4.0
  perpendicular = <=4.0
   parallell = <=3.0: no
   parallell = >3.0: yes
  perpendicular = >4.0
   parallell = <=3.0: no
   parallell = >3.0: no
 nbroflines = >4.0
  perpendicular = <=4.0
   parallell = <=3.0: no
   parallell = >3.0: yes
  perpendicular = >4.0
   parallell = <=3.0: yes
   parallell = >3.0: yes
avglength = >0.774125
 nbroflines = <=4.0
  perpendicular = <=4.0
   parallell = <=3.0: no
   parallell = >3.0: no
  perpendicular = >4.0: no
 nbroflines = >4.0: no

\end{lstlisting}

The tree is defined by spaces, each space signifies a branch.

\section{DISCUSSION}

\subsection{Parser}
The parser was relatively simple to develop, based on standard transformations from polar coordinates to Cartesian coordinates. The only problems we had was specific to the laser range scanner. At first we assumed that points was listed in clockwise order but it was actually listed in counter-clockwise order according to mathematical notation.

The biggest problem to overcome with the parser was that we only knew the angle between two adjacent points and not the absolute angle for each point. This led us to express the angle for each point as the absolute angle from the first point. Since we wanted to make our parser work for any laser range scanner we could not assume where the first point was located. Thus, our solution was to express the angle for each point as the angle between the point and the y-axis that we were to create.

However, other than this we did not have any major trouble with the parser.

\subsection{Algorithms}

\subsubsection{Interpolation and Comparison}
A stated above we realized that this algorithm would not work and we never finished the implementation. There were several reasons for this. First of all, the datasets that we received from our parser were all simply a set of points. Potentially the different measurements that we were going to store could come from different laser range scanners. Thus, a certain number of points could describe two differently sized fields of view. This is the main reason why we introduced interpolation so that we could compare the same points on the two different measurements.

The next problem to solve was that we realized that in principle we needed to overlay each measurement on the other and then slide them over each other in order to minimize the error between them. However, even the slightest shift could introduce huge errors. We never came up with a solution to this problem and instead we chose to abandon this approach and research other ways of classifying measurements.

\subsubsection{Line finding}
There are quite a lot of parameters that require tweaking in the line finding algorithm. The reasoning behind these parameters will be explained here.

\begin{description}
\item[0.25 m length of line:] \hfill \\ This parameter was chosen so as to not mistake feet for lines, since on quite a lot of the datasets a couple of feet will turn up and most of the furniture has legs as well.
\item[more than three points:] \hfill \\ To determine a line more than 3 points has to be chosen, this parameter was chosen to be more than three so that the algorithm does not draw lines over huge gaps which consists of 2-3 points.
\item[0.1 thick line:] \hfill \\ This parameter was chosen through testing to determine lines, we found that a 0.1 m line was quite a good value.
\item[10 degrees for perpendicular and parallel:] \hfill \\ To find the classifiers for a perpendicular and parallel line this value was chosen through testing. 
\end{description}

This algorithm requires quite a lot of tuning which leads to a lot of testing of the data and finding what fits for the most general case. The algorithm in itself is not that powerful, since it approximates quite a lot, however this lead to a faster running time which might be more convenient for online searching on the robot.
Even though it is a fast running algorithm it produces quite a bad result and it is quite difficult to expand on it to find other geometrical figures. This means that different algorithms has to be written to find other figures. In the end it might have been better to use a different algorithm to find lines. Although due to time limitation we did not have time to explore this route and we did not have time to tweak our line finding algorithm to the optimum.


\subsubsection{Decision Tree}
The decision tree we produce works fairly well on doors, however we could have used a lot of more classifiers and doing a one-vs-rest approach with decision trees as a comparing algorithm. However we felt like we did not have time to do all this so we focused on getting one thing right.

\subsection{Improvements}
There are several improvements that we have thought about in this project. The most obvious ones are the things we did not have time to implement. Furthermore, there are some more improvements that we have philosophized about.

In order for our classifier to work as it was intended, to identify different types of objects, we need to implement the one-vs-rest classifier as discussed above. As stated above we did not have time to implement more than one classifier. Thus, the improvement is to train several more decision trees to be used in the classifier by combining them using the one-vs-rest technique.

Another clear improvement is to implement more attributes and characteristics to detect in the measurement. The idea here is to use more than 4 attributes that can be gathered from the lines. Possibly we could use attributes such as the number of corners, number of corners that mirror each other and similar attributes. Also, an obvious improvement here is to extend the attributes to not just lines but to other patterns. For example, circles, rectangles and other shapes could be identified and suitable abstract attributes involving these shapes could be used as input to the decision trees.

A nice feature to have in the classifier is unsupervised learning. This improvement can be quite easily implemented by regenerating each decision tree once a new classification has been made. The classification would in this case be added to the examples that is used to generate the trees.

The last obvious improvement is to implement the classifier as a ROS node so that it could be used online on the robot. This was part of our original goal with the project but unfortunately we did not have time to fulfill that specific goal.

These improvements are those that would make the classifier somewhat useful if combined together. The robot could travel around and at specific intervals it could output a classification of what it currently sees. For example, if it detects a door it outputs that fact and if it detects a human it would state that. The classifications can be sent to the programs that dictate movement so that, for example, if a human would suddenly appear, the robot would stop.

However, we do have some improvements that we believe might increase the performance of the classifier.

Firstly, the line finding algorithm could be exchanged for a much better algorithm that focuses on clusters and from there define different characteristics such as lines or circles. The main problem with our algorithm is that it is directly dependent on the index of each point in the measurement. We strongly believe that this improvement would have a major impact on the performance of the classifier as it exists now.

Secondly, we have thought about another improvement which stems directly from the fact that our initial approach failed. The problem was that we worked with individual points. The success came when we instead looked at lines and essentially raised the level of abstraction. Thus, we have philosophized about increasing the level of abstraction even further and introduce more layers to the classifier. For example, we use one classifier that looks at characteristics of lines and determine if certain shapes occur. One classifier could identify rectangles another triangles and so forth. Then the classifiers that actually detects doors, humans, open spaces e.t.c. base their decisions  on the output of these intermediate classifiers. This improvement is something we have just fantasized about and we can not in any way predict how the performance would be effected but it would be very interesting to test.

\section{CONCLUSION}
During this project we have learned several lessons. Most of the lessons have come from the fact that we did not have a good idea of how to approach the problem in the beginning. Therefore, we attempted one approach that eventually proved to be to difficult to solve. However, in the end we did find an approach that we believe in but because we did not have much time left we could not implement is as fully as we would have liked.

We learned quite late in the project that it was far better to raise the abstraction of the problem. This was hinted to us earlier in the project by our supervisor but we had no idea of how to approach the problem in that way. It was not after we had realized that the first approach was a failure that we were forced to make further research in order to find our next approach which made use of a higher abstraction level. It was here that we gained most knowledge, we realized that we could actually use decision trees to do pattern recognition. This was something that we would never have come up with from the beginning. However, even though we had found an approach that seemed pretty good there was not much time left before the end of the project. Therefore, we could only implement the simplest possible solution that actually yielded some result. The results were not astonishing but we believe that if the improvements listed above were to be implemented the results would be better.

The biggest pitfalls in our project was that we did not put as much effort into research as we should have, at least not in the beginning. It was not until our second research phase that we actually found something we could use. Furthermore, we made one big error and that was to implement our own line finding algorithm. Had we done research on that we would have produced much better results. The biggest reason for us not doing this research was because of the limited time we had left but also we thought that it was easy to identify lines and it never really occured to us that we could have researched line finding algorithms.

The main conclusion of this project is that it is better to approach this problem by looking at a higher level than on individual points. This meant that an algorithm could take pretty abstract attributes as input in order to solve the problem.

We did not manage to meet all our goals and this was primarily because of the limited time. We would like to have been able to classify several different kinds of objects as well as being able to run the classifier online on the robot using ROS. We would also have liked to use unsupervised learning to let the classifier improve on itself.

Overall we have learned much in this project. When we first started working on it we had no clue on how to solve it but in the end we managed to produce some results even if they are not as good as we would have liked.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section*{APPENDIX}

%Appendixes should appear before the acknowledgment.

%\section*{ACKNOWLEDGMENT}

%The preferred spelling of the word �acknowledgment� in America is without an �e� after the �g�. Avoid the stilted expression, �One of us (R. B. G.) thanks . . .�  Instead, try �R. B. G. thanks�. Put sponsor acknowledgments in the unnumbered footnote on the first page.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%References are important to the reader; therefore, each citation must be complete and correct. If at all possible, references should be commonly available publications.



\begin{thebibliography}{}
\bibitem{interpolation} here goes interpolation source

\bibitem{legfinding}Xavier, Joao. Pacheco, Marco. Castro, Daniel. Ruano Antonio. Nunues, Urbano. Fast Line, Arc/Circle and Leg Detection from Laser Scan Data in a Player Driver, http://miarn.sourceforge.net/pdf/a1738b.pdf
\bibitem{ml3} Mitchell, Tom M. Machine Learning. 1997. Chapter 3.
\bibitem{praml} Bishop, Christopher M. 2006. Pattern Recognition and Machine Learning. Chapter 4, section 1.2.
\bibitem{aima} Russel, Stuart. Norvig, Peter. Artificial Intelligence: A Modern Approach. Third edition. 2010. Chapter 18. 
\end{thebibliography}




\end{document}

